{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# practice using Pytorch with Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv('data/train.csv', dtype=np.float32)\n",
    "\n",
    "# split data into pixels and labels  (our features are the pixels)\n",
    "targets_numpy = train['label'].values\n",
    "\n",
    "# features will be all categories except our target, and then standardize all values\n",
    "features_numpy = train.loc[:, train.columns != 'label'].values / 255\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_numpy, targets_numpy, test_size=0.2, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "# convert target to type long\n",
    "y_train_tensor = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_test_tensor = torch.from_numpy(y_test).type(torch.LongTensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "29"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch size, epoch, and iterations\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 10000\n",
    "epochs = int(n_iters / (len(X_train) / batch_size))\n",
    "epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_loader = DataLoader(train, batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: PYDEVD_USE_CYTHON environment variable is set to 'NO'. Frame evaluator will be also disabled because it requires Cython extensions to be enabled in order to operate correctly.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(-0.5, 27.5, 27.5, -0.5)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize an image in dataset\n",
    "plt.imshow(features_numpy[69].reshape(28,28))\n",
    "plt.axis('off')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# create regression model\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "\n",
    "        ## linear layer\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "        ## we place a logistic layer later\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# size of image px by px\n",
    "input_dim = 28*28\n",
    "# we 0-9 labels\n",
    "output_dim = 10\n",
    "\n",
    "model = LogisticRegressionModel(input_dim, output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500  Loss: 1.8434792757034302  Accuracy: 65.02381134033203%\n",
      "Iteration: 1000  Loss: 1.6226650476455688  Accuracy: 74.36904907226562%\n",
      "Iteration: 1500  Loss: 1.2879797220230103  Accuracy: 78.26190185546875%\n",
      "Iteration: 2000  Loss: 1.1992069482803345  Accuracy: 79.91666412353516%\n",
      "Iteration: 2500  Loss: 1.0462743043899536  Accuracy: 81.05952453613281%\n",
      "Iteration: 3000  Loss: 0.9324048757553101  Accuracy: 82.01190185546875%\n",
      "Iteration: 3500  Loss: 0.9103773236274719  Accuracy: 82.61904907226562%\n",
      "Iteration: 4000  Loss: 0.75664883852005  Accuracy: 83.07142639160156%\n",
      "Iteration: 4500  Loss: 0.9632409811019897  Accuracy: 83.42857360839844%\n",
      "Iteration: 5000  Loss: 0.8143795132637024  Accuracy: 83.75%\n",
      "Iteration: 5500  Loss: 0.7567724585533142  Accuracy: 84.04762268066406%\n",
      "Iteration: 6000  Loss: 0.872061550617218  Accuracy: 84.44047546386719%\n",
      "Iteration: 6500  Loss: 0.6622310876846313  Accuracy: 84.60713958740234%\n",
      "Iteration: 7000  Loss: 0.7113801836967468  Accuracy: 84.8452377319336%\n",
      "Iteration: 7500  Loss: 0.6370571851730347  Accuracy: 85.08333587646484%\n",
      "Iteration: 8000  Loss: 0.7444236874580383  Accuracy: 85.3452377319336%\n",
      "Iteration: 8500  Loss: 0.5431105494499207  Accuracy: 85.48809814453125%\n",
      "Iteration: 9000  Loss: 0.6454664468765259  Accuracy: 85.5952377319336%\n",
      "Iteration: 9500  Loss: 0.5234761238098145  Accuracy: 85.71428680419922%\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (image, labels) in enumerate(train_loader):\n",
    "        # Define variables\n",
    "        train = Variable(image.view(-1, 28 * 28))\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        # clear gradients after each iter\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #fp\n",
    "        outputs = model(train)\n",
    "\n",
    "        # loss func\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # bp\n",
    "        loss.backward()\n",
    "\n",
    "        #update\n",
    "        optimizer.step()\n",
    "\n",
    "        count+=1\n",
    "\n",
    "         # Prediction\n",
    "        if count % 50 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Predict test dataset\n",
    "            for images, labels in test_loader:\n",
    "                test = Variable(images.view(-1, 28*28))\n",
    "\n",
    "                # Forward propagation\n",
    "                outputs = model(test)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "\n",
    "                # Total number of labels\n",
    "                total += len(labels)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / float(total)\n",
    "\n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "        if count % 500 == 0:\n",
    "            # Print Loss\n",
    "            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-301d774e",
   "language": "python",
   "display_name": "PyCharm (API_Mini_Project)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}